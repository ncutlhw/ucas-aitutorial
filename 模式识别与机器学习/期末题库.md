# 模式识别与机器学习 期末模拟题库

> 说明：本题库基于课程笔记内容整理生成，题型包含单选题、多选题和判断题，覆盖关联分析、聚类、决策树、贝叶斯和集成学习等核心章节。

## 一、单选题（共10题）

1. **(关联分析) 在关联规则挖掘中，如果有规则 $X \Rightarrow Y$，其置信度（Confidence）的定义是（ ）。**
   A. $P(X \cup Y)$
   B. $P(Y|X)$
   C. $P(X|Y)$
   D. $P(Y)$

2. **(关联分析) Apriori 算法的核心思想利用了先验原理进行剪枝，以下描述正确的是（ ）。**
   A. 频繁项集的超集一定是频繁的
   B. 非频繁项集的子集一定是非频繁的
   C. 非频繁项集的超集一定是非频繁的
   D. 频繁项集的补集一定是频繁的

3. **(聚类) 以下关于 k-means 算法缺点的描述中，不正确的是（ ）。**
   A. 需要预先指定簇的个数 k
   B. 对离群点（噪声）非常敏感
   C. 只能发现球状簇
   D. 计算复杂度随样本量呈指数级增长

4. **(聚类) DBSCAN 算法中，如果一个点对象的 $\epsilon$-邻域内包含的样本点数大于等于 MinPts，则该点被称为（ ）。**
   A. 边界点
   B. 核心点
   C. 噪声点
   D. 密度直达点

5. **(决策树) ID3 算法在选择划分属性时，采用的准则是（ ）。**
   A. 信息增益
   B. 信息增益率
   C. 基尼系数
   D. 均方误差

6. **(决策树) C4.5 算法相比于 ID3 算法，主要改进了以下哪点？（ ）。**
   A. 只能处理二分类问题
   B. 克服了信息增益偏向于选择取值较多属性的不足
   C. 无法处理缺失值
   D. 生成的树是二叉树

7. **(决策树) CART 算法在构建分类树时，默认使用的分裂指标是（ ）。**
   A. 熵
   B. 信息增益
   C. 基尼系数 (Gini Index)
   D. 误分类率

8. **(贝叶斯) 朴素贝叶斯分类器的“朴素”假设是指（ ）。**
   A. 假设所有特征对分类同样重要
   B. 假设特征之间相互独立
   C. 假设先验概率相等
   D. 假设数据服从高斯分布

9. **(贝叶斯) 在参数估计中，最大后验估计 (MAP) 与最大似然估计 (MLE) 的主要区别在于 MAP 引入了（ ）。**
   A. 似然函数
   B. 参数的先验概率
   C. 损失函数
   D. 梯度下降

10. **(集成学习) AdaBoost 算法在每轮迭代中，对训练样本权重的调整策略是（ ）。**
    A. 降低被正确分类样本的权重，提高被错误分类样本的权重
    B. 提高被正确分类样本的权重，降低被错误分类样本的权重
    C. 所有样本权重保持不变
    D. 随机调整样本权重

---

## 二、多选题（共5题）

1. **(关联分析) 关联规则挖掘的两个核心步骤是（ ）。**
   A. 数据清洗
   B. 找出所有频繁项集
   C. 由频繁项集生成强关联规则
   D. 模型评估

2. **(聚类) 以下属于基于密度的聚类算法有（ ）。**
   A. k-means
   B. DBSCAN
   C. OPTICS
   D. AGNES (凝聚层次聚类)

3. **(决策树) 决策树模型防止过拟合的方法包括（ ）。**
   A. 增加树的深度
   B. 预剪枝 (Pre-pruning)
   C. 后剪枝 (Post-pruning)
   D. 限制叶子节点的最小样本数

4. **(贝叶斯) 关于贝叶斯决策理论，以下说法正确的有（ ）。**
   A. 属于生成式模型
   B. 基于后验概率进行决策
   C. 可以处理多分类问题
   D. 不需要先验概率

5. **(集成学习) 集成学习中常用的结合策略包括（ ）。**
   A. Bagging (如随机森林)
   B. Boosting (如 AdaBoost, GBDT)
   C. Stacking
   D. K-Fold

---

## 三、判断题（共5题）

1. **( )** Apriori 算法中，如果项集 $I$ 是频繁项集，则 $I$ 的所有非空子集也必须是频繁项集。

2. **( )** k-medoids 算法采用簇中位置最中心的实际对象作为参照点，因此比 k-means 对异常值更敏感。

3. **( )** 决策树构建过程中，信息增益越大，表示该属性对样本集的纯度提升越明显。

4. **( )** 朴素贝叶斯分类器不需要训练过程，直接计算概率即可。

5. **( )** GBDT (Gradient Boosting Decision Tree) 是通过拟合损失函数的负梯度来训练每一个新的基学习器的。

---

## 参考答案与解析

### 一、单选题
1. **B**。置信度 $Confidence(X \Rightarrow Y) = P(Y|X) = \frac{Support(X \cup Y)}{Support(X)}$。
2. **C**。Apriori 性质：频繁项集的非空子集一定是频繁的；非频繁项集的超集一定是非频繁的（用于剪枝）。
3. **D**。k-means 复杂度是 $O(nkt)$，通常是线性的，不是指数级。缺点包括对初值敏感、对噪声敏感、需指定 k。
4. **B**。DBSCAN 定义核心点为 $\epsilon$-邻域内至少包含 MinPts 个样本的点。
5. **A**。ID3 使用信息增益。
6. **B**。C4.5 使用信息增益率，解决了 ID3 偏向取值多属性的问题，并能处理连续值。
7. **C**。CART 分类树使用基尼系数，回归树使用平方误差（方差）。
8. **B**。朴素贝叶斯假设特征之间条件独立。
9. **B**。MAP = MLE + $\ln P(\theta)$，引入了参数的先验分布，相当于加了正则项。
10. **A**。AdaBoost 核心机制是关注被分错的样本，提高其权重。

### 二、多选题
1. **BC**。关联挖掘两步：1. 找频繁项集；2. 生成强关联规则。
2. **BC**。k-means 是划分方法，AGNES 是层次方法。
3. **BCD**。A 会加重过拟合。
4. **ABC**。贝叶斯决策需要先验概率 $P(C)$ 和类条件概率 $P(X|C)$。
5. **ABC**。集成学习三大类：Bagging, Boosting, Stacking。

### 三、判断题
1. **√**。这是 Apriori 算法的基础原理。
2. **×**。k-medoids 选用实际点作为中心，对异常值**不**敏感；k-means 用均值，对异常值敏感。
3. **√**。信息增益 = 熵 - 条件熵，增益越大，不确定性减少越多，纯度提升越明显。
4. **×**。朴素贝叶斯需要训练，训练过程就是统计先验概率和条件概率（参数估计）。
5. **√**。GBDT 利用负梯度近似残差。

