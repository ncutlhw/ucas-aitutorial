# 模式识别与机器学习 期末刷题题库

> 说明：本题库包含约100道题目，覆盖了课程的主要知识点，旨在帮助大家进行期末复习和刷题。

---

## 第一部分：单选题（共60题）

### 1. 关联分析

1. **在关联规则挖掘中，规则 $\{A\} \Rightarrow \{B\}$ 的支持度（Support）计算公式是（ ）。**
   A. $P(A \cup B)$
   B. $P(B|A)$
   C. $P(A|B)$
   D. $P(A) \times P(B)$

2. **若规则 $\{A\} \Rightarrow \{B\}$ 的置信度（Confidence）为 80%，意味着（ ）。**
   A. 80% 的事务包含 A 和 B
   B. 在包含 A 的事务中，有 80% 也包含 B
   C. 在包含 B 的事务中，有 80% 也包含 A
   D. A 和 B 出现的概率之和为 80%

3. **Apriori 算法利用先验原理（Apriori Property）进行剪枝，该原理指出（ ）。**
   A. 频繁项集的超集一定是频繁的
   B. 非频繁项集的超集一定是非频繁的
   C. 频繁项集的补集一定是频繁的
   D. 非频繁项集的子集一定是频繁的

4. **FP-growth 算法相比于 Apriori 算法的主要优势在于（ ）。**
   A. 不需要扫描数据库
   B. 不需要生成候选集
   C. 能够发现更多的关联规则
   D. 算法实现更简单

5. **FP-growth 算法在构建 FP-tree 时，需要扫描数据库的次数是（ ）。**
   A. 1次
   B. 2次
   C. k次（k为最大频繁项集的长度）
   D. 不确定

6. **在 FP-growth 算法中，为了挖掘包含某个特定项的频繁模式，需要构建（ ）。**
   A. 原始 FP-tree
   B. 逆序 FP-tree
   C. 条件模式基（Conditional Pattern Base）和条件 FP-tree
   D. 哈希树

7. **如果我们发现规则 $A \Rightarrow B$ 的提升度（Lift）小于 1，这说明（ ）。**
   A. A 和 B 是正相关的
   B. A 和 B 是相互独立的
   C. A 和 B 是负相关的（互斥的）
   D. 规则无效

8. **Apriori 算法在生成 $k$ 项频繁集时，利用了（ ）项频繁集。**
   A. $1$
   B. $k-2$
   C. $k-1$
   D. 所有小于 $k$ 的频繁集

9. **以下哪种方法不属于关联规则挖掘算法？（ ）**
   A. Apriori
   B. FP-growth
   C. Eclat
   D. DBSCAN

10. **在购物篮分析中，“啤酒与尿布”的故事反映了哪种数据挖掘技术的应用？（ ）**
    A. 分类分析
    B. 聚类分析
    C. 关联分析
    D. 预测分析

### 2. 聚类分析

11. **K-means 算法的核心思想是（ ）。**
    A. 基于密度的聚类
    B. 最小化簇内平方误差和（SSE）
    C. 层次化的合并簇
    D. 基于概率分布的聚类

12. **K-means 算法的一个主要缺点是（ ）。**
    A. 对大数据集收敛慢
    B. 需要预先指定簇的数量 k
    C. 只能处理二维数据
    D. 总是能找到全局最优解

13. **以下哪种聚类算法对异常值（Outliers）最不敏感？（ ）**
    A. K-means
    B. K-medoids (PAM)
    C. 层次聚类（使用均值距离）
    D. 高斯混合模型 (GMM)

14. **DBSCAN 算法中，如果一个点对象的 $\epsilon$-邻域内包含的样本点数 $\ge$ MinPts，则该点被称为（ ）。**
    A. 核心点
    B. 边界点
    C. 噪声点
    D. 离群点

15. **DBSCAN 算法的主要优点不包括（ ）。**
    A. 不需要预先指定簇的数量
    B. 能发现任意形状的簇
    C. 对噪声不敏感
    D. 对参数 $\epsilon$ and MinPts 不敏感

16. **凝聚层次聚类（Agglomerative Hierarchical Clustering）采用的策略是（ ）。**
    A. 自顶向下
    B. 自底向上
    C. 随机划分
    D. 划分-合并

17. **在 EM 算法应用于高斯混合模型（GMM）聚类时，E 步（Expectation step）主要计算（ ）。**
    A. 每个高斯分量的参数（均值、协方差）
    B. 每个样本属于各个高斯分量的后验概率（Responsibility）
    C. 似然函数的值
    D. 混合系数

18. **离群点（Outlier）通常被定义为（ ）。**
    A. 频繁出现的模式
    B. 与其他观测值显著不同的观测值
    C. 簇的中心点
    D. 边界上的点

19. **以下哪个指标通常不用于聚类内部有效性评估？（ ）**
    A. 轮廓系数 (Silhouette Coefficient)
    B. 簇内距离
    C. 簇间距离
    D. 准确率 (Accuracy)

20. **关于 K-means++ 算法，它是为了解决 K-means 的什么问题？（ ）**
    A. 确定 k 值的问题
    B. 初始中心点选择不当导致的局部最优问题
    C. 计算复杂度过高的问题
    D. 处理非球形簇的问题

### 3. 决策树

21. **ID3 算法在选择分裂属性时，使用的是（ ）。**
    A. 信息增益 (Information Gain)
    B. 信息增益率 (Gain Ratio)
    C. 基尼系数 (Gini Index)
    D. 方差减少量

22. **C4.5 算法引入“信息增益率”是为了解决 ID3 算法偏向于选择（ ）的特征的问题。**
    A. 取值较少
    B. 取值较多
    C. 连续值
    D. 缺失值

23. **CART (Classification and Regression Tree) 算法生成的决策树是（ ）。**
    A. 二叉树
    B. 多叉树
    C. 只有一层的树
    D. 不确定的

24. **对于分类问题，CART 算法使用的分裂准则是（ ）。**
    A. 熵
    B. 基尼系数
    C. 平方误差
    D. 似然函数

25. **决策树剪枝（Pruning）的主要目的是（ ）。**
    A. 减少训练时间
    B. 增加树的深度
    C. 防止过拟合，提高泛化能力
    D. 提高在训练集上的准确率

26. **在决策树中，如果一个节点包含的样本全部属于同一类别，那么该节点的熵（Entropy）为（ ）。**
    A. 0
    B. 1
    C. 0.5
    D. 无穷大

27. **预剪枝（Pre-pruning）是指在（ ）进行剪枝。**
    A. 决策树生成之后
    B. 决策树生成过程中
    C. 训练集划分之前
    D. 测试阶段

28. **以下关于决策树处理连续属性的说法，正确的是（ ）。**
    A. 决策树无法处理连续属性
    B. 需要先将连续属性离散化（如二分法）
    C. ID3 可以直接处理连续属性
    D. 处理连续属性时不需要计算信息增益

29. **基尼系数（Gini Index）越小，表示集合的纯度（ ）。**
    A. 越低
    B. 越高
    C. 不变
    D. 无法判断

30. **随机森林（Random Forest）通过（ ）来提高模型的准确率和稳定性。**
    A. 剪枝
    B. 增加树的深度
    C. 集成多个决策树（Bagging）
    D. 使用 Boosting 技术

### 4. 贝叶斯与KNN

31. **朴素贝叶斯（Naive Bayes）分类器的核心假设是（ ）。**
    A. 样本分布服从高斯分布
    B. 特征之间相互独立
    C. 这是一个线性分类器
    D. 先验概率相等

32. **在贝叶斯公式 $P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$ 中，$P(X|Y)$ 被称为（ ）。**
    A. 先验概率 (Prior)
    B. 后验概率 (Posterior)
    C. 似然概率 (Likelihood)
    D. 边缘概率 (Evidence)

33. **如果某个特征值在训练集中没有出现过，导致概率为 0，通常采用（ ）来解决。**
    A. 删除该特征
    B. 拉普拉斯平滑 (Laplace Smoothing)
    C. 将概率设为 0.5
    D. 增加训练样本

34. **最大似然估计 (MLE) 的目标是找到参数 $\theta$，使得（ ）最大化。**
    A. $P(\theta)$
    B. $P(\theta|D)$
    C. $P(D|\theta)$
    D. $P(D)$

35. **最大后验估计 (MAP) 在最大似然估计 (MLE) 的基础上引入了（ ）。**
    A. 损失函数
    B. 参数的先验分布 $P(\theta)$
    C. 正则化项 L1
    D. 动量项

36. **KNN (K-Nearest Neighbors) 算法属于（ ）学习。**
    A. 懒惰学习 (Lazy Learning)
    B. 积极学习 (Eager Learning)
    C. 无监督学习
    D. 强化学习

37. **在 KNN 算法中，k 值过小会导致模型（ ）。**
    A. 欠拟合 (Underfitting)
    B. 过拟合 (Overfitting)
    C. 偏差变大
    D. 计算量变小

38. **KD 树（K-Dimension Tree）的主要作用是（ ）。**
    A. 构建决策树
    B. 加速 KNN 的搜索过程
    C. 进行降维
    D. 聚类

39. **欧氏距离（Euclidean Distance）对应于闵可夫斯基距离的（ ）范数。**
    A. L1
    B. L2
    C. $L_\infty$
    D. L0

40. **曼哈顿距离（Manhattan Distance）对应于闵可夫斯基距离的（ ）范数。**
    A. L1
    B. L2
    C. $L_\infty$
    D. L0

### 5. 神经网络与线性模型

41. **单层感知器（Perceptron）无法解决（ ）问题。**
    A. 与 (AND)
    B. 或 (OR)
    C. 非 (NOT)
    D. 异或 (XOR)

42. **BP 神经网络的核心训练算法是（ ）。**
    A. 最小二乘法
    B. 误差反向传播（结合梯度下降）
    C. 期望最大化 (EM)
    D. 遗传算法

43. **Sigmoid 激活函数的一个主要缺点是（ ）。**
    A. 输出不是 0 均值
    B. 计算简单
    C. 在饱和区容易导致梯度消失
    D. 只能处理正数

44. **ReLU (Rectified Linear Unit) 激活函数的公式是（ ）。**
    A. $\frac{1}{1+e^{-x}}$
    B. $\tanh(x)$
    C. $\max(0, x)$
    D. $x$

45. **逻辑回归（Logistic Regression）虽然名字带“回归”，但实际上主要用于（ ）任务。**
    A. 线性回归
    B. 二分类
    C. 聚类
    D. 降维

46. **逻辑回归使用的损失函数通常是（ ）。**
    A. 均方误差 (MSE)
    B. 交叉熵损失 (Cross-Entropy Loss)
    C. Hinge Loss
    D. 0-1 Loss

47. **万能逼近定理（Universal Approximation Theorem）指出，包含（ ）个隐藏层的前馈神经网络可以逼近任意连续函数。**
    A. 0
    B. 至少 1
    C. 至少 2
    D. 至少 10

48. **Hebb 学习规则认为，如果两个神经元（ ），它们之间的连接权重应该增强。**
    A. 距离很近
    B. 同时激活
    C. 一个激活一个抑制
    D. 都不激活

49. **在神经网络训练中，动量（Momentum）项的主要作用是（ ）。**
    A. 防止过拟合
    B. 加速收敛并减少震荡
    C. 减少计算量
    D. 初始化权重

50. **卷积神经网络 (CNN) 中，卷积层的主要作用是（ ）。**
    A. 降维
    B. 提取特征
    C. 分类
    D. 防止过拟合

### 6. 集成学习与遗传算法

51. **Bagging (Bootstrap Aggregating) 的核心思想是（ ）。**
    A. 串行训练多个分类器，每个关注前一个分错的样本
    B. 并行训练多个分类器，通过自助采样（Bootstrap）获取训练集
    C. 训练一个强分类器
    D. 使用层级结构

52. **AdaBoost 算法属于（ ）集成策略。**
    A. Bagging
    B. Boosting
    C. Stacking
    D. Blending

53. **AdaBoost 在每一轮迭代中，会（ ）被前一轮分类器错误分类的样本的权重。**
    A. 增加
    B. 减少
    C. 保持不变
    D. 随机改变

54. **GBDT (Gradient Boosting Decision Tree) 的核心在于利用（ ）来拟合残差。**
    A. 样本权重
    B. 损失函数的负梯度
    C. 损失函数的二阶导数
    D. 随机特征选择

55. **XGBoost 相比于 GBDT，在目标函数中引入了（ ），并利用了损失函数的二阶泰勒展开。**
    A. 正则化项
    B. 动量项
    C. 学习率
    D. 交叉熵

56. **遗传算法（Genetic Algorithm）中，模拟生物进化的三个主要操作是（ ）。**
    A. 编码、解码、评估
    B. 梯度下降、反向传播、更新
    C. 选择、交叉、变异
    D. 聚类、分类、回归

57. **遗传算法中的“变异”操作的主要目的是（ ）。**
    A. 加快收敛速度
    B. 维持种群多样性，防止陷入局部最优
    C. 提高适应度
    D. 选择优秀个体

58. **随机森林（Random Forest）在构建决策树时，除了对样本进行采样，还对（ ）进行了随机采样。**
    A. 树的深度
    B. 节点分裂的特征候选集
    C. 损失函数
    D. 学习率

59. **Stacking 集成学习通常包含（ ）层模型。**
    A. 1
    B. 2 (基学习器 + 元学习器)
    C. 10
    D. 无数

60. **如果基分类器之间是（ ）的，集成学习的效果通常最好。**
    A. 完全相同
    B. 相互独立且具有差异性
    C. 准确率都很低
    D. 线性相关

---

## 第二部分：多选题（共20题）

61. **关联规则挖掘中，常用的算法有（ ）。**
    A. Apriori
    B. FP-growth
    C. K-means
    D. Eclat

62. **数据预处理中，处理缺失值的方法包括（ ）。**
    A. 直接删除包含缺失值的记录
    B. 使用均值、中位数或众数填充
    C. 使用预测模型填补
    D. 将缺失值作为一个单独的类别

63. **以下属于无监督学习算法的有（ ）。**
    A. K-means
    B. DBSCAN
    C. 逻辑回归
    D. 主成分分析 (PCA)

64. **聚类分析中，基于距离的聚类算法（如 K-means）容易受到以下哪些因素的影响？（ ）**
    A. 初始中心的选择
    B. 噪声和异常值
    C. 数据量纲（Feature Scaling）
    D. 样本的标签

65. **决策树构建过程中，停止分裂的条件通常包括（ ）。**
    A. 当前节点包含的样本全属于同一类别
    B. 当前节点的属性集为空（无法再分）
    C. 树达到了预设的最大深度
    D. 当前节点样本数少于预设阈值

66. **以下关于过拟合（Overfitting）的描述，正确的有（ ）。**
    A. 模型在训练集上表现很好，但在测试集上表现差
    B. 模型过于复杂，记住了训练数据的噪声
    C. 可以通过增加训练数据来缓解
    D. 可以通过正则化来缓解

67. **贝叶斯参数估计中，以下说法正确的有（ ）。**
    A. MLE 将参数视为固定的未知常数
    B. MAP 将参数视为服从某种先验分布的随机变量
    C. 当样本量趋于无穷大时，MAP 趋向于 MLE
    D. MAP 相当于在 MLE 基础上加了正则项

68. **KNN 算法中的距离度量方式包括（ ）。**
    A. 欧氏距离
    B. 曼哈顿距离
    C. 切比雪夫距离
    D. 闵可夫斯基距离

69. **神经网络中，常用的激活函数有（ ）。**
    A. Sigmoid
    B. Tanh
    C. ReLU (Rectified Linear Unit)
    D. Linear

70. **关于 BP 算法（反向传播算法），以下说法正确的有（ ）。**
    A. 基于梯度下降策略
    B. 需要计算损失函数对权重的偏导数
    C. 容易陷入局部极小值
    D. 只能用于浅层网络

71. **逻辑回归与线性回归的区别在于（ ）。**
    A. 逻辑回归用于分类，线性回归用于回归
    B. 逻辑回归使用了 Sigmoid 函数将输出映射到 (0,1)
    C. 逻辑回归通常使用交叉熵损失，线性回归使用均方误差
    D. 逻辑回归不能使用梯度下降

72. **集成学习中，Boosting 家族的代表算法有（ ）。**
    A. AdaBoost
    B. GBDT
    C. XGBoost
    D. Random Forest

73. **随机森林（Random Forest）具有以下哪些优点？（ ）**
    A. 能够处理高维数据
    B. 训练速度快，容易并行化
    C. 抗过拟合能力强
    D. 对噪声不敏感

74. **遗传算法的三个基本操作是（ ）。**
    A. 编码
    B. 选择 (Selection)
    C. 交叉 (Crossover)
    D. 变异 (Mutation)

75. **数据标准化（Normalization）常用的方法有（ ）。**
    A. 最小-最大规范化 (Min-Max Scaling)
    B. Z-Score 标准化
    C. 小数定标规范化
    D. 独热编码 (One-Hot Encoding)

76. **以下属于基于密度的聚类算法的是（ ）。**
    A. DBSCAN
    B. OPTICS
    C. DENCLUE
    D. K-means

77. **评估分类模型性能的常用指标有（ ）。**
    A. 准确率 (Accuracy)
    B. 精确率 (Precision)
    C. 召回率 (Recall)
    D. F1-Score

78. **关于支持向量机 (SVM)，以下说法正确的是（ ）。**
    A. 寻找最大间隔超平面
    B. 属于监督学习
    C. 可以使用核技巧处理非线性问题
    D. 目标是最小化结构风险

79. **关联规则评价标准除了支持度和置信度，还有（ ）。**
    A. 提升度 (Lift)
    B. 余弦相似度
    C. 全置信度 (All-confidence)
    D. Jaccard 系数

80. **FP-growth 算法挖掘频繁项集的过程包括（ ）。**
    A. 构建 FP-tree
    B. 产生候选集
    C. 构建条件模式基
    D. 递归挖掘条件 FP-tree

---

## 第三部分：判断题（共20题）

81. **( )** Apriori 算法通过多次扫描数据库来计算候选项集的支持度，因此I/O开销较大。

82. **( )** FP-growth 算法在整个挖掘过程中，不需要产生任何候选项集。

83. **( )** K-means 算法对初始聚类中心的选择敏感，不同的初始中心可能导致不同的聚类结果。

84. **( )** DBSCAN 算法可以发现任意形状的簇，并且不需要预先指定簇的个数。

85. **( )** 决策树的预剪枝（Pre-pruning）通常比后剪枝（Post-pruning）更能保留树的细节，因此泛化能力更强。

86. **( )** ID3 算法倾向于选择取值较多的属性作为分裂属性，这通常是一个缺点。

87. **( )** 朴素贝叶斯分类器假设所有特征之间是相互独立的，这在现实中很少成立，但它在很多任务上依然表现良好。

88. **( )** KNN 算法是一种“懒惰学习”算法，因为它在训练阶段基本不进行计算，直到预测时才进行计算。

89. **( )** 逻辑回归是一个回归算法，预测输出是连续值。

90. **( )** 感知器（Perceptron）只能解决线性可分问题，对于异或（XOR）这种非线性问题无能为力。

91. **( )** 增加神经网络的隐藏层层数，一定能提高模型的准确率。

92. **( )** Bagging 算法（如随机森林）各个基学习器之间存在强依赖关系，必须串行生成。

93. **( )** Boosting 算法（如 AdaBoost）通过调整样本权重，让模型更关注那些难以分类的样本。

94. **( )** GBDT 使用的是分类树作为基学习器，不能用于回归任务。

95. **( )** 遗传算法能保证一定找到全局最优解。

96. **( )** 轮廓系数（Silhouette Coefficient）接近 1 表示聚类效果较好，接近 -1 表示聚类效果较差。

97. **( )** 贝叶斯网络（Bayesian Network）是一种有向无环图（DAG），用于表示变量间的依赖关系。

98. **( )** L1 正则化倾向于产生稀疏解（即很多参数变为0），有助于特征选择。

99. **( )** 梯度消失问题通常发生在深层网络中，使用 ReLU 激活函数可以缓解这一问题。

100. **( )** 两个变量相关性高（Correlation高），意味着它们之间一定存在因果关系。

---

## 参考答案

### 一、单选题
1-5: ABABB
6-10: CCBDC
11-15: BBBAA (15:DBSCAN对参数敏感)
16-20: BBBDB
21-25: ABABC
26-30: ABBCC (29:Gini越小纯度越高)
31-35: BCBCB
36-40: ABBBB
41-45: DBCCB
46-50: BBBBB
51-55: BBABA
56-60: CBBBB

### 二、多选题
61. ABD (K-means是聚类)
62. ABCD
63. ABD (逻辑回归是监督)
64. ABC
65. ABCD
66. ABCD
67. ABCD
68. ABCD
69. ABC
70. ABC
71. ABC
72. ABC
73. ABC
74. BCD
75. AB (CD不是标准化)
76. ABC
77. ABCD
78. ABC (D是SVM特点)
79. ACD
80. ACD (不产生候选集)

### 三、判断题
81. √
82. √
83. √
84. √
85. × (预剪枝容易欠拟合，后剪枝泛化能力通常更好)
86. √
87. √
88. √
89. × (逻辑回归是分类算法)
90. √
91. × (可能过拟合或梯度消失)
92. × (Bagging是并行的，Boosting是串行的)
93. √
94. × (GBDT使用的是回归树，即使是分类任务也是拟合残差/概率)
95. × (启发式搜索，不保证全局最优)
96. √
97. √
98. √
99. √
100. × (相关不等于因果)

