# 强化学习

> 下学期还有专门的**强化学习**课程，慢慢来

# 一、基本概念

## 1.1 MDP五元组

![MDP](pngs/MDP.png)

## 1.2 强化学习的目标

![RL_object](pngs/RL_object.png)

## 1.3 Q函数、V函数

![Q&V](pngs/Q&V.png)
> **Q(s,a)**: 处于状态s时，采取动作a，能够获得的期望奖励。  
> **V(s)**: 处于状态s时，能够获得的期望奖励

## 1.4 贝尔曼方程

#### 1) 从这张图来看，更直观：

![Bellman_equations](pngs/Bellman_equations.png)
> 上图中使用的奖励值为：$R_s^a=E[r_t|s_t=s, a_t=a]$  
> 与课件中的$R_{ss'}^a$，所代表的含义有差异。

#### 2) 贝尔曼最优方程

![Bellman_optimality_equations](pngs/Bellman_optimality_equations.png)

## 1.5 几种强化学习算法

![RL_algorithms](pngs/RL_algorithms.png)

# 二、优化算法

## 2.1 策略梯度

![PG](pngs/PG.png)

进一步，$r_{sa}$可以用$Q^{\pi_\theta}(s,a)$替代：

![PG_2](pngs/PG_2.png)

### 2.1.1 $Q^{\pi_\theta}(s,a)$

**蒙特卡洛法来近似**：  
利用累计奖励值$G_t$作为$Q^{\pi_\theta}(s,a)$的无偏采样。

![REINFORCE](pngs/REINFORCE.png)

### 2.1.2 $\pi_\theta(a|s)$

> **使用softmax随机策略**  
> 这部分没看懂，略

## 2.2 Actor-Critic

### 2.2.1 AC（Actor-Critic）

![Actor_Critic](pngs/Actor_Critic.png)

### 2.2.2 A2C（Advantageous Actor-Critic）

> 添加一个baseline，使用优势函数$A(s,a)$来代替**AC**中的$Q(s,a)$

---
`后面都看不懂了，下学期再来～`

### 2.2.3 A3C（Asynchronous Advantage Actor Critic）

## 2.3 TRPO

## 2.4 PPO

## 2.5 GRPO
